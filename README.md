# ğŸ›¡ï¸ Toxic Comment Classification (LSTM + GloVe)

This project implements a **Toxic Comment Detector** using:
- Pretrained **GloVe word embeddings**
- **LSTM neural network** for text classification
- **Multilabel classification** (toxic, severe_toxic, obscene, threat, insult, identity_hate)
- **Streamlit app** for interactive testing

The model predicts whether a given comment contains toxic or harmful language.

---

## ğŸ“Œ Features
- Train an **LSTM with GloVe embeddings**
- Multilabel classification (6 toxicity categories)
- Save & reuse the **Tokenizer** (`tokenizer.pkl`)
- Streamlit app for real-time comment analysis
- Adjustable toxicity threshold

---

## ğŸ“‚ Project Structure
```
toxic_comment_classifier/
â”œâ”€â”€ app/                  # Application code (if you have a front-end or API)
â”œâ”€â”€ data/                 # Dataset files (e.g., raw data, processed data)
â”œâ”€â”€ glove/                # GloVe embedding files and related scripts
â”‚   â””â”€â”€ glove.6B.100d.txt
â”œâ”€â”€ glove_lstm_pipeline/  # Scripts for the GloVe + LSTM model
â”œâ”€â”€ models/               # Saved trained models
â”œâ”€â”€ notebooks/            # Jupyter notebooks for exploration and prototyping
â”œâ”€â”€ Reports/              # Reports, visualizations, and a project summary
â”œâ”€â”€ transformer_pipeline/ # Scripts for the Transformer-based model
â”œâ”€â”€ utils/                # Utility scripts (e.g., Model_evaluation)
â”œâ”€â”€ .gitignore            # Files and folders to ignore in Git
â”œâ”€â”€ check_len.py          # A script for checking text length
â”œâ”€â”€ requirements.txt      # Project dependencies
â”œâ”€â”€ README.md             # This filets.txt
â”œâ”€â”€ main_glove_lstm.py
```

---
ğŸ‹ï¸ Training
Run the training pipeline:

bash
Copy code
python main_glove_lstm.py
This will:

Train the LSTM with GloVe embeddings

Save the best model â†’ models/glove_lstm_best.keras

Save the final model â†’ models/glove_lstm_final.keras

Save the tokenizer â†’ models/tokenizer.pkl

ğŸ”® Inference
Use the saved model & tokenizer to predict toxicity:

python
Copy code
import pickle
import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Load model + tokenizer
model = tf.keras.models.load_model("models/glove_lstm_final.keras")
with open("models/tokenizer.pkl", "rb") as f:
    tokenizer = pickle.load(f)

def predict_toxicity(text, model, tokenizer, max_len=320):
    seq = tokenizer.texts_to_sequences([text])
    padded = pad_sequences(seq, maxlen=max_len, padding="post")
    prediction = model.predict(padded, verbose=0)
    return prediction[0]  # [toxic, severe_toxic, obscene, threat, insult, identity_hate]

print(predict_toxicity("You are an idiot!", model, tokenizer))
ğŸ›ï¸ Streamlit App
Run the app:

bash
Copy code
streamlit run app/streamlit_app.py
Features:

Enter a comment â†’ classify as Toxic / Non-Toxic

Shows toxicity probability score

Adjustable threshold

Sidebar with sample comments

ğŸ“¦ Requirements
requirements.txt

nginx
Copy code
tensorflow
numpy
pandas
streamlit
scikit-learn
plotly
ğŸš€ Deployment Notes
Always ship glove_lstm_final.keras and tokenizer.pkl together.

Keep the preprocessing consistent between training and inference.

Model outputs 6 probabilities (0â€“1), one for each toxicity label.

ğŸ“Œ Credits
Dataset: Jigsaw Toxic Comment Classification

Embeddings: GloVe (Stanford NLP)

Frameworks: TensorFlow / Keras, Streamlit
